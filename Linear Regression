1)	Calories_consumed-> predict weight gained using calories consumed

calories=read.csv(file.choose())
View(calories)
summary(calories)

> summary(calories)
 Weight.gained..grams. Calories.Consumed
 Min.   :  62.0        Min.   :1400     
 1st Qu.: 114.5        1st Qu.:1728     
 Median : 200.0        Median :2250     
 Mean   : 357.7        Mean   :2341     
 3rd Qu.: 537.5        3rd Qu.:2775     
 Max.   :1100.0        Max.   :3900   

plot(calories$Calories.Consumed,calories$Weight.gained..grams.,main="Scatter Plot", col="Dodgerblue4", 
     col.main="Dodgerblue4", col.lab="Dodgerblue4", xlab="Calories Consumed", 
     ylab="Weight Gained", pch=20)  # plot(x,y)

 

attach(calories)
cor(Calories.Consumed,Weight.gained..grams.)

cor(Calories.Consumed,Weight.gained..grams.)
[1] 0.946991


reg <- lm(Weight.gained..grams.~Calories.Consumed, data=calories)
summary(reg)

lm(formula = Weight.gained..grams. ~ Calories.Consumed, data = calories)

Residuals:
    Min      1Q  Median      3Q     Max 
-158.67 -107.56   36.70   81.68  165.53 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)       -625.75236  100.82293  -6.206 4.54e-05 ***
Calories.Consumed    0.42016    0.04115  10.211 2.86e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 111.6 on 12 degrees of freedom
Multiple R-squared:  0.8968,	Adjusted R-squared:  0.8882 
F-statistic: 104.3 on 1 and 12 DF,  p-value: 2.856e-07

confint(reg,level=0.95)

> confint(reg,level=0.95)
                         2.5 %       97.5 %
(Intercept)       -845.4266546 -406.0780569
Calories.Consumed    0.3305064    0.5098069


confint(reg,level=0.95)
pred <- predict(reg,interval="predict")

pred <- as.data.frame(pred)
View(pred)
cor(pred$fit,calories$Weight.gained..grams.)

cor(pred$fit,calories$Weight.gained..grams.)
[1] 0.946991

Both the p-values for the intercept and the predictor variable are highly significant, so we can reject the null hypothesis and accept the alternative hypothesis, which means that there is a significant association between the predictor and the outcome variables. It has Good relationship between them.

2)	Delivery_time -> Predict delivery time using sorting time
delivery=read.csv(file.choose())
delivery
summary(delivery)

> summary(delivery)
 Delivery.Time    Sorting.Time  
 Min.   : 8.00   Min.   : 2.00  
 1st Qu.:13.50   1st Qu.: 4.00  
 Median :17.83   Median : 6.00  
 Mean   :16.79   Mean   : 6.19  
 3rd Qu.:19.75   3rd Qu.: 8.00  
 Max.   :29.00   Max.   :10.00  

plot(delivery$Sorting.Time,delivery$Delivery.Time,main="Scatter Plot", col="Dodgerblue4", 
     col.main="Dodgerblue4", col.lab="Dodgerblue4", xlab="Sorting Time", 
     ylab="Delivery Time", pch=20)  # plot(x,y)

 

attach(delivery)
cor(Delivery.Time,Sorting.Time)

cor(Delivery.Time,Sorting.Time)
[1] 0.8259973

reg <- lm(Sorting.Time~Delivery.Time, data=delivery)
summary(reg)

Call:
lm(formula = Sorting.Time ~ Delivery.Time, data = delivery)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.1388 -1.0014 -0.1045  0.5521  3.3507 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -0.75667    1.13395  -0.667    0.513    
Delivery.Time  0.41374    0.06477   6.387 3.98e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.47 on 19 degrees of freedom
Multiple R-squared:  0.6823,	Adjusted R-squared:  0.6655 
F-statistic:  40.8 on 1 and 19 DF,  p-value: 3.983e-06

confint(reg,level = .95)
pred <- predict(reg,interval="predict")

pred <- as.data.frame(pred)
pred
cor(pred$fit, delivery$Sorting.Time)

cor(pred$fit, delivery$Sorting.Time)
[1] 0.8259973

reg_sqrt <- lm(Delivery.Time~sqrt(Sorting.Time), data=delivery)
summary(reg_sqrt)
confint(reg,level = .95)
pred <- predict(reg,interval="predict")

reg_log<-lm(Delivery.Time~log(Sorting.Time), data=delivery)
summary(reg_log)

confint(reg_log,level=0.95)
predict(reg_log,interval="predict")

reg1<-lm(log(Delivery.Time)~Sorting.Time + I(Sorting.Time*Sorting.Time), data=wc.at)
summary(reg1)
Call:
lm(formula = log(Delivery.Time) ~ Sorting.Time + I(Sorting.Time * 
    Sorting.Time), data = wc.at)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.21194 -0.11776 -0.03034  0.10550  0.35975 

Coefficients:
                               Estimate Std. Error t value Pr(>|t|)    
(Intercept)                     1.69970    0.22843   7.441 6.77e-07 ***
Sorting.Time                    0.26592    0.08022   3.315  0.00385 ** 
I(Sorting.Time * Sorting.Time) -0.01284    0.00632  -2.032  0.05722 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1627 on 18 degrees of freedom
Multiple R-squared:  0.7649,	Adjusted R-squared:  0.7387 
F-statistic: 29.28 on 2 and 18 DF,  p-value: 2.197e-06


confint(reg1,level=0.95)
predict(reg1,interval="predict")
pred<-predict(reg1,interval="predict")

pred<-as.data.frame(pred)
View(pred)

exp(pred$fit)
cor(exp(pred$fit),delivery$Delivery.Time)

exp(pred$fit)
 [1] 21.646921 12.909227 16.995933 21.176938 21.646921 16.995933 18.764526 10.825502 21.646921
[10] 21.176938 20.191891 12.909227 18.764526 10.825502 10.825502 12.909227 16.995933 18.764526
[19]  8.847949 18.764526 15.003730
> cor(exp(pred$fit),delivery$Delivery.Time)
[1] 0.8258883

Both the p-values for the intercept and the predictor variable are not significant, so we can accept the null hypothesis and reject the alternative hypothesis, which means that there is no significant association between the predictor and the outcome variables. It’s not a good model fit.

3)	Salary_hike -> Build a prediction model for Salary_hike
salary_data=read.csv(file.choose())
salary_data
summary(salary_data)

summary(salary_data)
 YearsExperience      Salary      
 Min.   : 1.100   Min.   : 37731  
 1st Qu.: 3.200   1st Qu.: 56721  
 Median : 4.700   Median : 65237  
 Mean   : 5.313   Mean   : 76003  
 3rd Qu.: 7.700   3rd Qu.:100545  
 Max.   :10.500   Max.   :122391 

plot(salary_data$Salary, salary_data$YearsExperience,
     main="Scatter Plot", col="Dodgerblue4", 
     col.main="Dodgerblue4", col.lab="Dodgerblue4", xlab="Years of Experience", 
     ylab="Salary", pch=20)  # plot(x,y)


 

attach(salary_data)
cor(YearsExperience, Salary)

cor(YearsExperience, Salary)
[1] 0.9782416
> reg <- lm(Salary~YearsExperience, data=salary_data)
> summary(reg)

Call:
lm(formula = Salary ~ YearsExperience, data = salary_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-7958.0 -4088.5  -459.9  3372.6 11448.0 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)      25792.2     2273.1   11.35 5.51e-12 ***
YearsExperience   9450.0      378.8   24.95  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5788 on 28 degrees of freedom
Multiple R-squared:  0.957,	Adjusted R-squared:  0.9554 
F-statistic: 622.5 on 1 and 28 DF,  p-value: < 2.2e-16

confint(reg,level = .95)
pred1 <- predict(reg,interval="predict")
pred1 <- as.data.frame(pred1)
pred1
View(pred1)
??predict


cor(pred1$fit,salary_data$Salary)

cor(pred1$fit,salary_data$Salary)
[1] 0.9782416


Both the p-values for the intercept and the predictor variable are highly significant, so we can reject the null hypothesis and accept the alternative hypothesis, which means that there is a significant association between the predictor and the outcome variables. It has Good relationship between them.
4)	Emp_data -> Build a prediction model for Churn_out_rate

emp_data=read.csv(file.choose())
emp_data
summary(emp_data)

plot(emp_data$Salary_hike,emp_data$Churn_out_rate,main="Scatter Plot", col="Dodgerblue4", 
     col.main="Dodgerblue4", col.lab="Dodgerblue4", xlab="Salary Hike", 
     ylab="Churn Out Rate", pch=20)  # plot(x,y)
attach(emp_data)
cor(Salary_hike,Churn_out_rate)

reg <- lm(Churn_out_rate~Salary_hike, data=emp_data)
summary(reg)
confint(reg,level = .95)
pred <- predict(reg,interval = "predict")
pred <- as.data.frame(pred)
pred
cor(pred$fit,emp_data$Churn_out_rate)

reg_sqrt <- lm(Churn_out_rate~sqrt(Salary_hike), data=emp_data)
summary(reg_sqrt)
confint(reg,level = .95)
predict(reg,interval = "predict")

reg_log<-lm(Churn_out_rate~log(Salary_hike), data=emp_data)
summary(reg_log)

confint(reg_log,level=0.95)
predict(reg_log,interval="predict")

reg1<-lm(log(Churn_out_rate)~Salary_hike + I(Salary_hike*Salary_hike), data=emp_data)
summary(reg1)
confint(reg1,level=0.95)
predict(reg1,interval="predict")
pred<-predict(reg1,interval="predict")

pred<-as.data.frame(pred)
View(pred)
exp(pred$fit)

cor(exp(pred$fit),emp_data$Churn_out_rate)


Call:
lm(formula = log(Churn_out_rate) ~ Salary_hike + I(Salary_hike * 
    Salary_hike), data = emp_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.027877 -0.014280  0.002735  0.012608  0.027882 

Coefficients:
                               Estimate Std. Error t value Pr(>|t|)    
(Intercept)                   2.318e+01  2.415e+00   9.597  2.8e-05 ***
Salary_hike                  -2.068e-02  2.813e-03  -7.351 0.000156 ***
I(Salary_hike * Salary_hike)  5.605e-06  8.175e-07   6.857 0.000241 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.01997 on 7 degrees of freedom
Multiple R-squared:  0.9836,	Adjusted R-squared:  0.9789 
F-statistic: 210.1 on 2 and 7 DF,  p-value: 5.634e-07

> confint(reg1,level=0.95)
                                     2.5 %        97.5 %
(Intercept)                   1.746563e+01  2.888684e+01
Salary_hike                  -2.733275e-02 -1.402780e-02
I(Salary_hike * Salary_hike)  3.672104e-06  7.538047e-06
> predict(reg1,interval="predict")
        fit      lwr      upr
1  4.493907 4.436877 4.550937
2  4.436784 4.383781 4.489786
3  4.409904 4.358135 4.461673
4  4.335990 4.285588 4.386393
5  4.292320 4.241615 4.343024
6  4.235222 4.183490 4.286953
7  4.208895 4.156666 4.261123
8  4.174785 4.122099 4.227471
9  4.112180 4.058933 4.165426
10 4.104504 4.040267 4.168741


pred<-as.data.frame(pred)
> View(pred)
> exp(pred$fit)
 [1] 89.47028 84.50273 82.26156 76.40060 73.13594 69.07699 67.28213 65.02585 61.07971 60.61269
> cor(exp(pred$fit),emp_data$Churn_out_rate)
[1] 0.9907286

Both the p-values for the intercept and the predictor variable are highly significant, so we can reject the null hypothesis and accept the alternative hypothesis, which means that there is a significant association between the predictor and the outcome variables. It has Good relationship between them.






